{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_5388\\1537572215.py:1: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv('Books.csv')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ISBN</th>\n",
       "      <th>Book-Title</th>\n",
       "      <th>Book-Author</th>\n",
       "      <th>Year-Of-Publication</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>Image-URL-S</th>\n",
       "      <th>Image-URL-M</th>\n",
       "      <th>Image-URL-L</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0195153448</td>\n",
       "      <td>Classical Mythology</td>\n",
       "      <td>Mark P. O. Morford</td>\n",
       "      <td>2002</td>\n",
       "      <td>Oxford University Press</td>\n",
       "      <td>http://images.amazon.com/images/P/0195153448.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0195153448.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0195153448.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0002005018</td>\n",
       "      <td>Clara Callan</td>\n",
       "      <td>Richard Bruce Wright</td>\n",
       "      <td>2001</td>\n",
       "      <td>HarperFlamingo Canada</td>\n",
       "      <td>http://images.amazon.com/images/P/0002005018.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0002005018.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0002005018.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0060973129</td>\n",
       "      <td>Decision in Normandy</td>\n",
       "      <td>Carlo D'Este</td>\n",
       "      <td>1991</td>\n",
       "      <td>HarperPerennial</td>\n",
       "      <td>http://images.amazon.com/images/P/0060973129.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0060973129.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0060973129.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0374157065</td>\n",
       "      <td>Flu: The Story of the Great Influenza Pandemic...</td>\n",
       "      <td>Gina Bari Kolata</td>\n",
       "      <td>1999</td>\n",
       "      <td>Farrar Straus Giroux</td>\n",
       "      <td>http://images.amazon.com/images/P/0374157065.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0374157065.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0374157065.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0393045218</td>\n",
       "      <td>The Mummies of Urumchi</td>\n",
       "      <td>E. J. W. Barber</td>\n",
       "      <td>1999</td>\n",
       "      <td>W. W. Norton &amp;amp; Company</td>\n",
       "      <td>http://images.amazon.com/images/P/0393045218.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0393045218.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0393045218.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ISBN                                         Book-Title  \\\n",
       "0  0195153448                                Classical Mythology   \n",
       "1  0002005018                                       Clara Callan   \n",
       "2  0060973129                               Decision in Normandy   \n",
       "3  0374157065  Flu: The Story of the Great Influenza Pandemic...   \n",
       "4  0393045218                             The Mummies of Urumchi   \n",
       "\n",
       "            Book-Author Year-Of-Publication                   Publisher  \\\n",
       "0    Mark P. O. Morford                2002     Oxford University Press   \n",
       "1  Richard Bruce Wright                2001       HarperFlamingo Canada   \n",
       "2          Carlo D'Este                1991             HarperPerennial   \n",
       "3      Gina Bari Kolata                1999        Farrar Straus Giroux   \n",
       "4       E. J. W. Barber                1999  W. W. Norton &amp; Company   \n",
       "\n",
       "                                         Image-URL-S  \\\n",
       "0  http://images.amazon.com/images/P/0195153448.0...   \n",
       "1  http://images.amazon.com/images/P/0002005018.0...   \n",
       "2  http://images.amazon.com/images/P/0060973129.0...   \n",
       "3  http://images.amazon.com/images/P/0374157065.0...   \n",
       "4  http://images.amazon.com/images/P/0393045218.0...   \n",
       "\n",
       "                                         Image-URL-M  \\\n",
       "0  http://images.amazon.com/images/P/0195153448.0...   \n",
       "1  http://images.amazon.com/images/P/0002005018.0...   \n",
       "2  http://images.amazon.com/images/P/0060973129.0...   \n",
       "3  http://images.amazon.com/images/P/0374157065.0...   \n",
       "4  http://images.amazon.com/images/P/0393045218.0...   \n",
       "\n",
       "                                         Image-URL-L  \n",
       "0  http://images.amazon.com/images/P/0195153448.0...  \n",
       "1  http://images.amazon.com/images/P/0002005018.0...  \n",
       "2  http://images.amazon.com/images/P/0060973129.0...  \n",
       "3  http://images.amazon.com/images/P/0374157065.0...  \n",
       "4  http://images.amazon.com/images/P/0393045218.0...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('Books.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning Data Function\n",
    "def clean_text(text):\n",
    "    cleaned = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "    return cleaned.strip().lower()\n",
    "# Clean the book titles\n",
    "data['Book-Title'] = data['Book-Title'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'Year-Of-Publication' column to numeric, forcing invalid values to NaN\n",
    "data['Year-Of-Publication'] = pd.to_numeric(data['Year-Of-Publication'], errors='coerce')\n",
    "\n",
    "# Drop rows where 'Year-Of-Publication' is NaN\n",
    "data = data.dropna(subset=['Year-Of-Publication'])\n",
    "\n",
    "# Filter books published in the year 2000 or later\n",
    "filtered_data = data[data['Year-Of-Publication'] == 2004]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ISBN</th>\n",
       "      <th>Book-Title</th>\n",
       "      <th>Book-Author</th>\n",
       "      <th>Year-Of-Publication</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>Image-URL-S</th>\n",
       "      <th>Image-URL-M</th>\n",
       "      <th>Image-URL-L</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0887841740</td>\n",
       "      <td>the middle stories</td>\n",
       "      <td>Sheila Heti</td>\n",
       "      <td>2004.0</td>\n",
       "      <td>House of Anansi Press</td>\n",
       "      <td>http://images.amazon.com/images/P/0887841740.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0887841740.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0887841740.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0971880107</td>\n",
       "      <td>wild animus</td>\n",
       "      <td>Rich Shapero</td>\n",
       "      <td>2004.0</td>\n",
       "      <td>Too Far</td>\n",
       "      <td>http://images.amazon.com/images/P/0971880107.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0971880107.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0971880107.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>0553584383</td>\n",
       "      <td>dead aim</td>\n",
       "      <td>IRIS JOHANSEN</td>\n",
       "      <td>2004.0</td>\n",
       "      <td>Bantam Books</td>\n",
       "      <td>http://images.amazon.com/images/P/0553584383.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0553584383.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0553584383.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>0316735736</td>\n",
       "      <td>all he ever wanted: a novel</td>\n",
       "      <td>Anita Shreve</td>\n",
       "      <td>2004.0</td>\n",
       "      <td>Back Bay Books</td>\n",
       "      <td>http://images.amazon.com/images/P/0316735736.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0316735736.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0316735736.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>140003180X</td>\n",
       "      <td>the kalahari typing school for men (no. 1 ladi...</td>\n",
       "      <td>ALEXANDER MCCALL SMITH</td>\n",
       "      <td>2004.0</td>\n",
       "      <td>Anchor</td>\n",
       "      <td>http://images.amazon.com/images/P/140003180X.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/140003180X.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/140003180X.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ISBN                                         Book-Title  \\\n",
       "12   0887841740                                 the middle stories   \n",
       "26   0971880107                                        wild animus   \n",
       "117  0553584383                                           dead aim   \n",
       "120  0316735736                        all he ever wanted: a novel   \n",
       "243  140003180X  the kalahari typing school for men (no. 1 ladi...   \n",
       "\n",
       "                Book-Author  Year-Of-Publication              Publisher  \\\n",
       "12              Sheila Heti               2004.0  House of Anansi Press   \n",
       "26             Rich Shapero               2004.0                Too Far   \n",
       "117           IRIS JOHANSEN               2004.0           Bantam Books   \n",
       "120            Anita Shreve               2004.0         Back Bay Books   \n",
       "243  ALEXANDER MCCALL SMITH               2004.0                 Anchor   \n",
       "\n",
       "                                           Image-URL-S  \\\n",
       "12   http://images.amazon.com/images/P/0887841740.0...   \n",
       "26   http://images.amazon.com/images/P/0971880107.0...   \n",
       "117  http://images.amazon.com/images/P/0553584383.0...   \n",
       "120  http://images.amazon.com/images/P/0316735736.0...   \n",
       "243  http://images.amazon.com/images/P/140003180X.0...   \n",
       "\n",
       "                                           Image-URL-M  \\\n",
       "12   http://images.amazon.com/images/P/0887841740.0...   \n",
       "26   http://images.amazon.com/images/P/0971880107.0...   \n",
       "117  http://images.amazon.com/images/P/0553584383.0...   \n",
       "120  http://images.amazon.com/images/P/0316735736.0...   \n",
       "243  http://images.amazon.com/images/P/140003180X.0...   \n",
       "\n",
       "                                           Image-URL-L  \n",
       "12   http://images.amazon.com/images/P/0887841740.0...  \n",
       "26   http://images.amazon.com/images/P/0971880107.0...  \n",
       "117  http://images.amazon.com/images/P/0553584383.0...  \n",
       "120  http://images.amazon.com/images/P/0316735736.0...  \n",
       "243  http://images.amazon.com/images/P/140003180X.0...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = filtered_data['Book-Title'].values\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(titles)\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "input_sequences = []\n",
    "for line in titles:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "\n",
    "# Pad sequences\n",
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "\n",
    "# Step 5: Prepare input and label data\n",
    "X = input_sequences[:, :-1]\n",
    "y = input_sequences[:, -1]\n",
    "# y = np.array(y)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y = tf.keras.utils.to_categorical(y, num_classes=total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Build the LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n",
    "model.add(LSTM(150))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "887/887 [==============================] - 47s 50ms/step - loss: 7.6266 - accuracy: 0.0598\n",
      "Epoch 2/50\n",
      "887/887 [==============================] - 47s 53ms/step - loss: 6.9768 - accuracy: 0.0908\n",
      "Epoch 3/50\n",
      "887/887 [==============================] - 53s 60ms/step - loss: 6.5037 - accuracy: 0.1249\n",
      "Epoch 4/50\n",
      "887/887 [==============================] - 65s 73ms/step - loss: 6.0011 - accuracy: 0.1519\n",
      "Epoch 5/50\n",
      "887/887 [==============================] - 65s 73ms/step - loss: 5.5056 - accuracy: 0.1811\n",
      "Epoch 6/50\n",
      "887/887 [==============================] - 60s 67ms/step - loss: 5.0419 - accuracy: 0.2053\n",
      "Epoch 7/50\n",
      "887/887 [==============================] - 64s 72ms/step - loss: 4.6196 - accuracy: 0.2346\n",
      "Epoch 8/50\n",
      "887/887 [==============================] - 52s 59ms/step - loss: 4.2229 - accuracy: 0.2710\n",
      "Epoch 9/50\n",
      "887/887 [==============================] - 59s 66ms/step - loss: 3.8546 - accuracy: 0.3166\n",
      "Epoch 10/50\n",
      "887/887 [==============================] - 57s 64ms/step - loss: 3.4981 - accuracy: 0.3713\n",
      "Epoch 11/50\n",
      "887/887 [==============================] - 62s 70ms/step - loss: 3.1634 - accuracy: 0.4268\n",
      "Epoch 12/50\n",
      "887/887 [==============================] - 75s 85ms/step - loss: 2.8503 - accuracy: 0.4792\n",
      "Epoch 13/50\n",
      "887/887 [==============================] - 67s 76ms/step - loss: 2.5638 - accuracy: 0.5296\n",
      "Epoch 14/50\n",
      "887/887 [==============================] - 69s 78ms/step - loss: 2.3066 - accuracy: 0.5769\n",
      "Epoch 15/50\n",
      "887/887 [==============================] - 63s 71ms/step - loss: 2.0802 - accuracy: 0.6194\n",
      "Epoch 16/50\n",
      "887/887 [==============================] - 62s 70ms/step - loss: 1.8784 - accuracy: 0.6565\n",
      "Epoch 17/50\n",
      "887/887 [==============================] - 69s 78ms/step - loss: 1.7002 - accuracy: 0.6926\n",
      "Epoch 18/50\n",
      "887/887 [==============================] - 67s 75ms/step - loss: 1.5447 - accuracy: 0.7196\n",
      "Epoch 19/50\n",
      "887/887 [==============================] - 58s 65ms/step - loss: 1.4064 - accuracy: 0.7455\n",
      "Epoch 20/50\n",
      "887/887 [==============================] - 65s 73ms/step - loss: 1.2863 - accuracy: 0.7658\n",
      "Epoch 21/50\n",
      "887/887 [==============================] - 64s 72ms/step - loss: 1.1862 - accuracy: 0.7823\n",
      "Epoch 22/50\n",
      "887/887 [==============================] - 71s 80ms/step - loss: 1.0958 - accuracy: 0.7981\n",
      "Epoch 23/50\n",
      "887/887 [==============================] - 68s 77ms/step - loss: 1.0194 - accuracy: 0.8100\n",
      "Epoch 24/50\n",
      "887/887 [==============================] - 66s 75ms/step - loss: 0.9563 - accuracy: 0.8203\n",
      "Epoch 25/50\n",
      "887/887 [==============================] - 67s 76ms/step - loss: 0.9008 - accuracy: 0.8279\n",
      "Epoch 26/50\n",
      "887/887 [==============================] - 59s 67ms/step - loss: 0.8555 - accuracy: 0.8333\n",
      "Epoch 27/50\n",
      "887/887 [==============================] - 56s 63ms/step - loss: 0.8165 - accuracy: 0.8382\n",
      "Epoch 28/50\n",
      "887/887 [==============================] - 61s 68ms/step - loss: 0.7853 - accuracy: 0.8416\n",
      "Epoch 29/50\n",
      "887/887 [==============================] - 69s 78ms/step - loss: 0.7577 - accuracy: 0.8438\n",
      "Epoch 30/50\n",
      "887/887 [==============================] - 71s 80ms/step - loss: 0.7374 - accuracy: 0.8441\n",
      "Epoch 31/50\n",
      "887/887 [==============================] - 71s 80ms/step - loss: 0.7182 - accuracy: 0.8460\n",
      "Epoch 32/50\n",
      "887/887 [==============================] - 54s 61ms/step - loss: 0.7036 - accuracy: 0.8466\n",
      "Epoch 33/50\n",
      "887/887 [==============================] - 60s 68ms/step - loss: 0.6906 - accuracy: 0.8471\n",
      "Epoch 34/50\n",
      "887/887 [==============================] - 65s 74ms/step - loss: 0.6806 - accuracy: 0.8472\n",
      "Epoch 35/50\n",
      "887/887 [==============================] - 72s 81ms/step - loss: 0.6719 - accuracy: 0.8487\n",
      "Epoch 36/50\n",
      "887/887 [==============================] - 71s 80ms/step - loss: 0.6636 - accuracy: 0.8480\n",
      "Epoch 37/50\n",
      "887/887 [==============================] - 64s 73ms/step - loss: 0.6587 - accuracy: 0.8485\n",
      "Epoch 38/50\n",
      "887/887 [==============================] - 65s 74ms/step - loss: 0.6522 - accuracy: 0.8480\n",
      "Epoch 39/50\n",
      "887/887 [==============================] - 65s 74ms/step - loss: 0.6484 - accuracy: 0.8478\n",
      "Epoch 40/50\n",
      "887/887 [==============================] - 64s 73ms/step - loss: 0.6437 - accuracy: 0.8485\n",
      "Epoch 41/50\n",
      "887/887 [==============================] - 68s 77ms/step - loss: 0.6399 - accuracy: 0.8476\n",
      "Epoch 42/50\n",
      "887/887 [==============================] - 61s 68ms/step - loss: 0.6371 - accuracy: 0.8469\n",
      "Epoch 43/50\n",
      "887/887 [==============================] - 66s 74ms/step - loss: 0.6339 - accuracy: 0.8482\n",
      "Epoch 44/50\n",
      "887/887 [==============================] - 68s 77ms/step - loss: 0.6297 - accuracy: 0.8490\n",
      "Epoch 45/50\n",
      "887/887 [==============================] - 75s 85ms/step - loss: 0.6296 - accuracy: 0.8484\n",
      "Epoch 46/50\n",
      "887/887 [==============================] - 56s 64ms/step - loss: 0.6275 - accuracy: 0.8479\n",
      "Epoch 47/50\n",
      "887/887 [==============================] - 60s 68ms/step - loss: 0.6239 - accuracy: 0.8483\n",
      "Epoch 48/50\n",
      "887/887 [==============================] - 60s 67ms/step - loss: 0.6220 - accuracy: 0.8487\n",
      "Epoch 49/50\n",
      "887/887 [==============================] - 59s 66ms/step - loss: 0.6213 - accuracy: 0.8487\n",
      "Epoch 50/50\n",
      "887/887 [==============================] - 83s 94ms/step - loss: 0.6193 - accuracy: 0.8488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has been trained and saved as lstm_model.h5\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Train the model\n",
    "early_stop = EarlyStopping(monitor='loss', patience=3)\n",
    "model.fit(X, y, epochs=50, callbacks=[early_stop], verbose=1)\n",
    "\n",
    "# Step 8: Save the model\n",
    "model.save('model_lstm.h5')\n",
    "\n",
    "print(\"Model has been trained and saved as lstm_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer_json = tokenizer.to_json()\n",
    "with open('tokenizer.json', 'w') as f:\n",
    "    json.dump(tokenizer_json, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suggestions based on input 'adventure':\n",
      "adventure in\n",
      "adventure in my\n",
      "adventure in my heart\n",
      "adventure in my heart aiken\n",
      "adventure in my heart aiken ginny\n",
      "adventure in my heart aiken ginny silver\n",
      "adventure in my heart aiken ginny silver hills\n",
      "adventure in my heart aiken ginny silver hills trilogy\n",
      "adventure in my heart aiken ginny silver hills trilogy the\n",
      "adventure in my heart aiken ginny silver hills trilogy the next\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import json\n",
    "\n",
    "# Load the trained model\n",
    "model = load_model('model_lstm.h5')\n",
    "\n",
    "# Load the tokenizer\n",
    "with open('tokenizer.json') as f:\n",
    "    tokenizer_json = json.load(f)\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import tokenizer_from_json\n",
    "tokenizer = tokenizer_from_json(tokenizer_json)\n",
    "\n",
    "# Function to generate book title suggestions based on user input\n",
    "def generate_suggestions(seed_text, num_suggestions=10):\n",
    "    max_sequence_len = model.input_shape[1] + 1  # Get max sequence length from the model input shape\n",
    "    suggestions = []\n",
    "    \n",
    "    for _ in range(num_suggestions):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "        predicted = model.predict(token_list, verbose=0)\n",
    "        predicted_word_index = np.argmax(predicted, axis=-1)\n",
    "        predicted_word = ''\n",
    "        \n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted_word_index:\n",
    "                predicted_word = word\n",
    "                break\n",
    "        \n",
    "        # Append the predicted word to the seed text\n",
    "        seed_text += \" \" + predicted_word\n",
    "        suggestions.append(seed_text)\n",
    "    \n",
    "    return suggestions\n",
    "\n",
    "# Example test\n",
    "user_input = \"adventure\"\n",
    "suggestions = generate_suggestions(user_input)\n",
    "\n",
    "print(\"Suggestions based on input '{}':\".format(user_input))\n",
    "for suggestion in suggestions:\n",
    "    print(suggestion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
